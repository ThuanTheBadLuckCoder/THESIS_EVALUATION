{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ChatBot Response Evaluation\n",
    "\n",
    "## Objective\n",
    "Comprehensively evaluate and compare three different RAG ChatBot responses to the same prompt using multiple evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File responses.txt not found.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Can you generate an HTML code using TailwindCSS that creates a responsive feature section with a main heading, a short description, and three feature cards? Each card should include an SVG icon, a title, a description, and a list of related topics (like Computers, Health, Reference). The layout should use a grid system and flexbox for alignment, styled entirely with Tailwind utility classes. It should have a clean, modern design and be fully responsive. This component should be static, with no JavaScript interactivity.\"\n",
    "\n",
    "# Read responses from a text file\n",
    "def read_responses_from_text_file(file_path):\n",
    "    try:\n",
    "        responses = {}\n",
    "        current_response = None\n",
    "        response_content = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Check if this is a response identifier line (e.g., \"Response A:\")\n",
    "                if line.endswith(':') and line.startswith('Response '):\n",
    "                    # If we were already processing a response, save it\n",
    "                    if current_response:\n",
    "                        responses[current_response] = '\\n'.join(response_content)\n",
    "                    \n",
    "                    # Start new response\n",
    "                    current_response = line[:-1]  # Remove the colon\n",
    "                    response_content = []\n",
    "                \n",
    "                # Otherwise add to current response content if we're in a response section\n",
    "                elif current_response:\n",
    "                    response_content.append(line)\n",
    "            \n",
    "            # Add the last response if there is one\n",
    "            if current_response and response_content:\n",
    "                responses[current_response] = '\\n'.join(response_content)\n",
    "                \n",
    "        return responses\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"responses.txt\"\n",
    "responses = read_responses_from_text_file(file_path)\n",
    "\n",
    "if responses:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nResponses:\")\n",
    "    for key, value in responses.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(value)\n",
    "        \n",
    "        \n",
    "# Define the prompt and three responses\n",
    "# prompt = \"Your specific prompt here\"\n",
    "# responses = {\n",
    "#     'Response A': \"First RAG response\",\n",
    "#     'Response B': \"Second RAG response\",\n",
    "#     'Response C': \"Third RAG response\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Semantic Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Semantic similarity to the prompt\u001b[39;00m\n\u001b[32m     10\u001b[39m semantic_scores = {}\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, response \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m     12\u001b[39m     semantic_scores[name] = semantic_similarity(prompt, response)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSemantic Similarity Scores:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "def semantic_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity using spaCy word embeddings\n",
    "    \"\"\"\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Semantic similarity to the prompt\n",
    "semantic_scores = {}\n",
    "for name, response in responses.items():\n",
    "    semantic_scores[name] = semantic_similarity(prompt, response)\n",
    "\n",
    "print(\"Semantic Similarity Scores:\")\n",
    "for name, score in semantic_scores.items():\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROUGE Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "Response A:\n",
      "  ROUGE-1: 0.1298\n",
      "  ROUGE-2: 0.0441\n",
      "  ROUGE-L: 0.0718\n",
      "Response B:\n",
      "  ROUGE-1: 0.0387\n",
      "  ROUGE-2: 0.0000\n",
      "  ROUGE-L: 0.0277\n",
      "Response C:\n",
      "  ROUGE-1: 0.1818\n",
      "  ROUGE-2: 0.1290\n",
      "  ROUGE-L: 0.1371\n"
     ]
    }
   ],
   "source": [
    "# ROUGE Score Calculation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = {}\n",
    "for name, response in responses.items():\n",
    "    rouge_scores[name] = scorer.score(prompt, response)\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "for name, scores in rouge_scores.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "    print(f\"  ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"  ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Scores:\n",
      "Response A: 17.0156\n",
      "Response B: 15.2050\n",
      "Response C: 23.2467\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Function to read responses from a text file\n",
    "def read_responses_from_file(file_path):\n",
    "    try:\n",
    "        responses = {}\n",
    "        current_response = None\n",
    "        response_content = []\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Check if this is a response identifier line (e.g., \"Response A:\")\n",
    "                if line.endswith(':') and line.startswith('Response '):\n",
    "                    # If we were already processing a response, save it\n",
    "                    if current_response:\n",
    "                        responses[current_response] = '\\n'.join(response_content)\n",
    "                    \n",
    "                    # Start new response\n",
    "                    current_response = line[:-1]  # Remove the colon\n",
    "                    response_content = []\n",
    "                \n",
    "                # Otherwise add to current response content if we're in a response section\n",
    "                elif current_response:\n",
    "                    response_content.append(line)\n",
    "            \n",
    "            # Add the last response if there is one\n",
    "            if current_response and response_content:\n",
    "                responses[current_response] = '\\n'.join(response_content)\n",
    "                \n",
    "        return responses\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def calculate_perplexity(text, max_length=1024):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of the text with chunking for long texts\n",
    "    Lower perplexity indicates better language model performance\n",
    "    \"\"\"\n",
    "    # Handle text that's too long by chunking\n",
    "    if len(text) > 3000:  # An arbitrary threshold\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        chunk = []\n",
    "        chunk_length = 0\n",
    "        \n",
    "        for word in words:\n",
    "            chunk.append(word)\n",
    "            chunk_length += len(word) + 1  # +1 for space\n",
    "            \n",
    "            if chunk_length > 1000:  # Smaller than max_length to be safe\n",
    "                chunks.append(' '.join(chunk))\n",
    "                chunk = []\n",
    "                chunk_length = 0\n",
    "        \n",
    "        # Add the last chunk if it exists\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk))\n",
    "        \n",
    "        # Calculate perplexity for each chunk and average\n",
    "        perplexities = []\n",
    "        for chunk_text in chunks:\n",
    "            # Prepare the input\n",
    "            encodings = tokenizer(chunk_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "            \n",
    "            # Calculate loss\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            perplexities.append(perplexity)\n",
    "        \n",
    "        return sum(perplexities) / len(perplexities)\n",
    "    else:\n",
    "        # Prepare the input\n",
    "        encodings = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "        \n",
    "        # Calculate loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "\n",
    "# Example usage\n",
    "file_path = \"responses.txt\"\n",
    "responses = read_responses_from_file(file_path)\n",
    "\n",
    "if responses:\n",
    "    # Calculate perplexity for each response\n",
    "    perplexity_scores = {}\n",
    "    for name, response in responses.items():\n",
    "        perplexity_scores[name] = calculate_perplexity(response)\n",
    "    \n",
    "    print(\"Perplexity Scores:\")\n",
    "    for name, score in perplexity_scores.items():\n",
    "        print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding-based Bias Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Admin/nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m bias_scores = {}\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, response \u001b[38;5;129;01min\u001b[39;00m responses.items():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     bias_scores[name] = \u001b[43mcalculate_word_embedding_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_terms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBias Scores:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, score \u001b[38;5;129;01min\u001b[39;00m bias_scores.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcalculate_word_embedding_bias\u001b[39m\u001b[34m(text, bias_terms)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mCalculate bias in word embeddings\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Calculate average similarity to bias terms\u001b[39;00m\n\u001b[32m     14\u001b[39m bias_scores = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ITITIU20316\\THESIS\\ROUGE\\THESIS_EVALUATION\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Admin/nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\ITITIU20316\\\\THESIS\\\\ROUGE\\\\THESIS_EVALUATION\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained word vectors\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "def calculate_word_embedding_bias(text, bias_terms):\n",
    "    \"\"\"\n",
    "    Calculate bias in word embeddings\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Calculate average similarity to bias terms\n",
    "    bias_scores = []\n",
    "    for term in bias_terms:\n",
    "        term_similarities = []\n",
    "        for token in tokens:\n",
    "            if token in word_vectors.key_to_index:\n",
    "                try:\n",
    "                    similarity = word_vectors.similarity(token, term)\n",
    "                    term_similarities.append(similarity)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        \n",
    "        # Average similarity for this bias term\n",
    "        if term_similarities:\n",
    "            bias_scores.append(np.mean(term_similarities))\n",
    "    \n",
    "    # Overall bias score\n",
    "    return np.mean(bias_scores) if bias_scores else 0\n",
    "\n",
    "# Define bias terms (these should be chosen carefully)\n",
    "bias_terms = ['man', 'woman', 'he', 'she', 'male', 'female']\n",
    "\n",
    "# Calculate bias for each response\n",
    "bias_scores = {}\n",
    "for name, response in responses.items():\n",
    "    bias_scores[name] = calculate_word_embedding_bias(response, bias_terms)\n",
    "\n",
    "print(\"Bias Scores:\")\n",
    "for name, score in bias_scores.items():\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all evaluation metrics\n",
    "evaluation_summary = pd.DataFrame({\n",
    "    'Semantic Similarity': semantic_scores,\n",
    "    'ROUGE-1 Score': {name: scores['rouge1'].fmeasure for name, scores in rouge_scores.items()},\n",
    "    'ROUGE-2 Score': {name: scores['rouge2'].fmeasure for name, scores in rouge_scores.items()},\n",
    "    'Perplexity': perplexity_scores,\n",
    "    'Embedding Bias': bias_scores\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Evaluation Summary:\")\n",
    "print(evaluation_summary)\n",
    "\n",
    "# Normalize and weight different metrics\n",
    "def normalize_column(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "# Weights for different metrics (adjust as needed)\n",
    "weights = {\n",
    "    'Semantic Similarity': 0.25,\n",
    "    'ROUGE-1 Score': 0.2,\n",
    "    'ROUGE-2 Score': 0.2,\n",
    "    'Perplexity': -0.2,  # Negative weight as lower is better\n",
    "    'Embedding Bias': -0.15  # Negative weight as less bias is better\n",
    "}\n",
    "\n",
    "# Normalize and weight the scores\n",
    "weighted_scores = {}\n",
    "for column, weight in weights.items():\n",
    "    normalized = normalize_column(evaluation_summary[column])\n",
    "    for name in responses.keys():\n",
    "        if name not in weighted_scores:\n",
    "            weighted_scores[name] = 0\n",
    "        weighted_scores[name] += normalized[name] * weight\n",
    "\n",
    "# Print final ranking\n",
    "print(\"\\nFinal Weighted Scores:\")\n",
    "for name, score in sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(weighted_scores.keys(), weighted_scores.values())\n",
    "plt.title('Comprehensive RAG Response Evaluation')\n",
    "plt.xlabel('Response')\n",
    "plt.ylabel('Weighted Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive evaluation of RAG ChatBot responses using multiple metrics:\n",
    "\n",
    "1. Semantic Similarity\n",
    "2. ROUGE Scores\n",
    "3. Perplexity\n",
    "4. Embedding-based Bias Evaluation\n",
    "\n",
    "The final weighted score considers the performance across these different dimensions, providing a holistic assessment of the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
